{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html, etree\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url_form = 'http://forum.spore.com/jforum/posts/list/{0}/{1}.page'\n",
    "page_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(offset, topic_id):\n",
    "    url = url_form.format(offset, topic_id)\n",
    "    \n",
    "    if topic_id in page_cache and offset//15 in page_cache[topic_id]:\n",
    "        page = page_cache[topic_id][offset // 15]\n",
    "    else:\n",
    "        page = requests.get(url)\n",
    "            \n",
    "    if offset is not 0:\n",
    "        page_cache[topic_id].append( page )\n",
    "    else:\n",
    "        page_cache[topic_id] = [page]\n",
    "    \n",
    "    return html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_id):\n",
    "    tree = make_tree(0, topic_id)\n",
    "    \n",
    "    page_numbers = [el.text_content() for el in tree.cssselect('.pagination a')]\n",
    "    # The last two pagination buttons are a \"next page\" arrow and \"Go\" button,\n",
    "    # so the actual last page number is third from the end.\n",
    "    # If the topic only has one page, there's no pagination buttons at all.\n",
    "    page_count = int(page_numbers[-3]) if page_numbers else 1\n",
    "\n",
    "    def scrape(offset, topic_id):\n",
    "        tree = make_tree(offset, topic_id) # TODO: rate limiter\n",
    "        print(offset//15 + 1, '/', page_count)\n",
    "        \n",
    "        usernames = [el.text_content() for el in tree.cssselect('.genmed')]\n",
    "        # if not usernames: return None  # if no usernames, this is a void topic. bail out!\n",
    "\n",
    "        dates = [el.text_content() for el in tree.cssselect('.date')]\n",
    "        post_ids = [el.cssselect('a')[1].get('name') for el in tree.cssselect('.postinfo')]\n",
    "        for i, post_id in enumerate(post_ids):\n",
    "            \n",
    "            root = tree.cssselect('#post_text_{}'.format(post_id))[0]\n",
    "            # for el in root.iter('blockquote'):\n",
    "                # if el.text: print(usernames[i], el.text_content().strip())\n",
    "                # if el.text is not None: el.text = '> ' + el.text # codecs.escape_decode(el.content)[0].decode()\n",
    "                    \n",
    "            content = root.cssselect('.postbody')\n",
    "            for el in content[0].iter('*'):\n",
    "                if el.text: el.text = el.text.strip('\\t\\n\\r\\f')\n",
    "                if el.tail: el.tail = el.tail.strip('\\t\\n\\r\\f')\n",
    "                if el.tag == 'textarea': \n",
    "                    el.text = el.text.replace('\\r\\n', '\\r') # ASSUME codeblock contains no tags\n",
    "                    \n",
    "            attach = root.cssselect('.gensmall')\n",
    "                    \n",
    "            yield {\n",
    "                'id': post_id,\n",
    "                'user': usernames[i],\n",
    "                'date': dates[i].strip(),\n",
    "                'content': etree.tostring(content[0], encoding='utf-8'),\n",
    "                'edits': etree.tostring(attach[0], encoding='utf-8') if attach else '',\n",
    "            }\n",
    "\n",
    "    ret = [scrape(j*15, topic_id) for j in range(0, page_count)] # make list of generators\n",
    "    \n",
    "    return [post for page in ret for post in page] # spool out all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_cols = ['id', 'user', 'date', 'content', 'edits']\n",
    "tsv_headers = '\\t'.join(tsv_cols)\n",
    "\n",
    "# FEED scraper in background1111\n",
    "# TODO: collect changes in title of thread from its replies\n",
    "\n",
    "topics = []\n",
    "print( len(topics) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,thread in enumerate(topics):\n",
    "    print('topic {} ({}/{})'.format(thread, i+1, len(topics)))\n",
    "    \n",
    "    with open('./{}.tsv'.format(thread), 'wb') as tsv:\n",
    "        tsv.write((tsv_headers + '\\n').encode('utf-8'))\n",
    "        topic = scrape_topic(thread)\n",
    "        \n",
    "        for post in topic:\n",
    "            for col in tsv_cols:\n",
    "                res = post[col] if isinstance(post[col], bytes) else post[col].encode('utf-8')\n",
    "                tsv.write(res)\n",
    "                tsv.write(('\\t' if col is not tsv_cols[-1] else '\\n').encode('utf-8'))\n",
    "                \n",
    "            # tsv_line = '\\t'.join([str(post[col]) for col in tsv_cols])            \n",
    "        # time.sleep(1)  # to avoid overwhelming the poor server :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test serialization roundtrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 20495, 22  # taxonomy shop\n",
    "# 45652, 1   # short-lived evolution game reboot\n",
    "# 300, 800   # autohotkey script to fix graphics?\n",
    "# 34634, 7148 # linux joke\n",
    "\n",
    "with open('./{}.tsv'.format(topics[0]), 'r') as f:\n",
    "    ret = list( csv.DictReader(f, delimiter='\\t') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,u in enumerate(ret):\n",
    "    print( i, u['user'], u['date'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "# strip markup\n",
    "#print( '\\n'.join([(el.text or '') + ' ' + (el.tail or '')\n",
    "#                  for el in etree.fromstring(ret[800]['content']).iter('*')]) )\n",
    "\n",
    "# TODO: trim tabs but not spaces.\n",
    "HTML(ret[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for root,dirs,files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tsv\"):\n",
    "            with open('./{}'.format(file), 'r') as f:\n",
    "                ret = list( csv.DictReader(f, delimiter='\\t') )\n",
    "                try:\n",
    "                    for line in ret:\n",
    "                        if 'textarea' in line['content']:\n",
    "                            print(file, 'codeblock')\n",
    "                            break\n",
    "                except:\n",
    "                    print(file, 'trouble')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ scrapy shell 'http://forum.spore.com/jforum/posts/listByUser/0/27337.page'\n",
    "\n",
    "# topics posted in by user\n",
    ">>> [s.split('.page') for s in response.css('.gen').extract()]\n",
    "# elements whose [1] ends in /topic-id, whose [2] ends in /post-id.\n",
    "\n",
    "$ scrapy shell 'http://forum.spore.com/jforum/posts/list/0/30084.page'\n",
    "\n",
    "# message ids by topic & offset\n",
    ">>> [s.split('post_id=') for s in response.css('.postinfo').extract()]\n",
    "# elements whose [1] begins with post-id.\n",
    "\n",
    "\n",
    "# message author by id\n",
    ">>> [s.split('.page') for s in response.css('.icon_profile').extract()]\n",
    "# VERIFY: elements whose [0] ends in /user-id.\n",
    "\n",
    "# message texts by id (only if present)\n",
    ">>> response.css('#post_text_898695').extract()\n",
    "# html of post body, given its post-id, topic-id *and* page-offset.\n",
    "\n",
    "# unless we can ask preList, must preprocess topics to assign offsets to messages. may as well collect all messages in the topic at this point.\n",
    "\n",
    "\n",
    "# given user-id as id,\n",
    "\n",
    "# given topic-id as id,\n",
    "    messageCount = 0 # MUST EXTRACT from navigation on first page\n",
    "    offsets = [d*15 for d in range(messageCount // 15)]\n",
    "    topicUrls = ['.../{}/{}.page'.format(d, id) for d in offsets]\n",
    "\n",
    "# given topic-id and page-offset,\n",
    "    messageIds = ...\n",
    "    messageAuthor = ... # associate to users csv\n",
    "    messageTexts = ...\n",
    "    for each triple, write to topic csv.\n",
    "\n",
    "# collect variation in topic title over time on same pass? index by message id.\n",
    "# variation in locations, siggies is lost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
